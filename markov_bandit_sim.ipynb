{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/bandit_simulator/blob/main/markov_bandit_sim.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Callable, Optional, Union\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(None)  # Use system entropy for true randomness\n",
    "random.seed(None)  # Use system entropy for true randomness\n",
    "\n",
    "# ================ MARKOV BANDIT ENVIRONMENT CLASSES ================\n",
    "\n",
    "class MarkovBanditEnvironment:\n",
    "    \"\"\"Base class for two-armed Markov bandit environments.\"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Markov bandit environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns in the game\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.exploration_bonus = exploration_bonus\n",
    "        self.states = None  # Hidden states (X_t^1, X_t^2)\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'states': [],\n",
    "            'exploration_bonus': []\n",
    "        }\n",
    "    \n",
    "    def reset(self, T: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Reset the environment for a new game.\n",
    "        \n",
    "        Args:\n",
    "            T (int, optional): Number of turns for the new game\n",
    "        \"\"\"\n",
    "        if T is not None:\n",
    "            self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'states': [],\n",
    "            'exploration_bonus': []\n",
    "        }\n",
    "        self._initialize_states()\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and action.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _compute_exploration_bonus(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the exploration bonus for the chosen action.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The exploration bonus\n",
    "        \"\"\"\n",
    "        # Default implementation (to be overridden by subclasses if needed)\n",
    "        # Simple count-based bonus that decreases with the number of times the arm has been pulled\n",
    "        action_counts = [self.history['actions'].count(0), self.history['actions'].count(1)]\n",
    "        if action_counts[action] == 0:\n",
    "            return self.exploration_bonus\n",
    "        return self.exploration_bonus / np.sqrt(action_counts[action])\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: The reward and exploration bonus obtained\n",
    "        \"\"\"\n",
    "        # Check if the episode has ended (considering random horizon if applicable)\n",
    "        if hasattr(self, 'random_horizon') and self.random_horizon:\n",
    "            if hasattr(self, 'actual_horizon') and self.current_turn >= self.actual_horizon:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        elif self.current_turn >= self.T:\n",
    "            raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        # Record current state\n",
    "        self.history['states'].append(self.states.copy())\n",
    "        \n",
    "        # Determine reward based on action and current state\n",
    "        reward = self._compute_reward(action)\n",
    "        \n",
    "        # Compute exploration bonus\n",
    "        exploration_bonus = self._compute_exploration_bonus(action)\n",
    "        \n",
    "        # Update history\n",
    "        self.history['actions'].append(action)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['exploration_bonus'].append(exploration_bonus)\n",
    "        \n",
    "        # Update states based on action\n",
    "        self._update_states(action)\n",
    "        \n",
    "        # Increment turn\n",
    "        self.current_turn += 1\n",
    "        \n",
    "        return reward, exploration_bonus\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        # By default, agents only see rewards, actions, turn info, and exploration bonus\n",
    "        return {\n",
    "            'current_turn': self.current_turn,\n",
    "            'total_turns': self.T,\n",
    "            'history': {\n",
    "                'actions': self.history['actions'],\n",
    "                'rewards': self.history['rewards'],\n",
    "                'exploration_bonus': self.history['exploration_bonus']\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class Problem1Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 1: Independent, Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Each arm follows an independent Markov process with known transition matrices.\n",
    "    The states are hidden from the agent, but the transition matrices are known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 1 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        self.random_horizon = False\n",
    "        \n",
    "        # Generate more contrasting reward values for states\n",
    "        if state_rewards is None:\n",
    "            # Create more contrasting rewards between good and bad states\n",
    "            base_low = 0.05 + np.random.random() * 0.2  # Low reward between 0.05-0.25\n",
    "            base_high = 0.75 + np.random.random() * 0.2  # High reward between 0.75-0.95\n",
    "            self.state_rewards = [base_low, base_high]\n",
    "        else:\n",
    "            self.state_rewards = state_rewards\n",
    "        \n",
    "        # Generate transition matrices for both arms\n",
    "        self.transition_matrices = [\n",
    "            self._generate_interesting_transition_matrix(state_count, arm_idx=i) for i in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _generate_interesting_transition_matrix(self, state_count: int, arm_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an interesting transition matrix with specific patterns.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "            arm_idx (int): Index of the arm (0 or 1) to generate different patterns\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((state_count, state_count))\n",
    "        \n",
    "        # Different patterns for different arms to create strategic diversity\n",
    "        if arm_idx == 0:\n",
    "            # First arm has \"sticky\" good state but unstable bad state\n",
    "            # Good state (high index) tends to stay in good state\n",
    "            # Bad state (low index) has higher chance to transition\n",
    "            for i in range(state_count):\n",
    "                # Higher state index = \"better\" state\n",
    "                if i == state_count - 1:  # Good state is sticky\n",
    "                    stay_prob = 0.7 + np.random.random() * 0.2  # 0.7-0.9 probability to stay\n",
    "                    matrix[i, i] = stay_prob\n",
    "                    # Distribute remaining probability\n",
    "                    remaining = 1.0 - stay_prob\n",
    "                    for j in range(state_count - 1):\n",
    "                        matrix[i, j] = remaining / (state_count - 1)\n",
    "                else:  # Bad states are less sticky\n",
    "                    stay_prob = 0.3 + np.random.random() * 0.2  # 0.3-0.5 probability to stay\n",
    "                    matrix[i, i] = stay_prob\n",
    "                    # Higher chance to go to better states\n",
    "                    remaining = 1.0 - stay_prob\n",
    "                    better_states_prob = remaining * 0.7  # 70% of remaining goes to better states\n",
    "                    better_states_count = state_count - i - 1\n",
    "                    \n",
    "                    if better_states_count > 0:\n",
    "                        for j in range(i + 1, state_count):\n",
    "                            matrix[i, j] = better_states_prob / better_states_count\n",
    "                    \n",
    "                    # Rest goes to worse states\n",
    "                    worse_states_prob = remaining * 0.3\n",
    "                    worse_states_count = i\n",
    "                    \n",
    "                    if worse_states_count > 0:\n",
    "                        for j in range(i):\n",
    "                            matrix[i, j] = worse_states_prob / worse_states_count\n",
    "        else:\n",
    "            # Second arm has more volatile transitions overall\n",
    "            # Creates a cyclical pattern with bias towards progression\n",
    "            for i in range(state_count):\n",
    "                # Cyclical pattern with forward bias\n",
    "                forward_idx = (i + 1) % state_count\n",
    "                backward_idx = (i - 1) % state_count\n",
    "                \n",
    "                # Forward probability higher than backward\n",
    "                forward_prob = 0.4 + np.random.random() * 0.2  # 0.4-0.6\n",
    "                backward_prob = 0.1 + np.random.random() * 0.2  # 0.1-0.3\n",
    "                stay_prob = 1.0 - forward_prob - backward_prob\n",
    "                \n",
    "                matrix[i, i] = stay_prob\n",
    "                matrix[i, forward_idx] = forward_prob\n",
    "                matrix[i, backward_idx] = backward_prob\n",
    "        \n",
    "        # Normalize rows to ensure they sum to 1\n",
    "        for i in range(state_count):\n",
    "            row_sum = np.sum(matrix[i])\n",
    "            if row_sum > 0:\n",
    "                matrix[i] = matrix[i] / row_sum\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _joint_state_to_individual(self, joint_state: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a joint state index to individual arm states.\n",
    "        \n",
    "        Args:\n",
    "            joint_state (int): Joint state index\n",
    "        \n",
    "        Returns:\n",
    "            List[int]: Individual states [state1, state2]\n",
    "        \"\"\"\n",
    "        state1 = joint_state // self.state_count\n",
    "        state2 = joint_state % self.state_count\n",
    "        return [state1, state2]\n",
    "    \n",
    "    def _individual_to_joint_state(self, states: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convert individual arm states to a joint state index.\n",
    "        \n",
    "        Args:\n",
    "            states (List[int]): Individual states [state1, state2]\n",
    "        \n",
    "        Returns:\n",
    "            int: Joint state index\n",
    "        \"\"\"\n",
    "        return states[0] * self.state_count + states[1]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        if self.joint_dynamics:\n",
    "            # Convert individual states to joint state\n",
    "            current_joint_state = self._individual_to_joint_state(self.states)\n",
    "            \n",
    "            # Get transition probabilities for the current joint state\n",
    "            transition_probabilities = self.joint_transition_matrix[current_joint_state]\n",
    "            \n",
    "            # Sample next joint state\n",
    "            next_joint_state = np.random.choice(self.joint_state_count, p=transition_probabilities)\n",
    "            \n",
    "            # Convert back to individual states\n",
    "            self.states = self._joint_state_to_individual(next_joint_state)\n",
    "        else:\n",
    "            # Update each arm's state independently\n",
    "            for arm in range(2):\n",
    "                current_state = self.states[arm]\n",
    "                transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "                self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        \n",
    "        # Add transition information if known\n",
    "        if self.known_transitions:\n",
    "            if self.joint_dynamics:\n",
    "                info['joint_transition_matrix'] = self.joint_transition_matrix\n",
    "                info['joint_dynamics'] = True\n",
    "            else:\n",
    "                info['transition_matrices'] = self.transition_matrices\n",
    "                info['joint_dynamics'] = False\n",
    "            \n",
    "            info['state_rewards'] = self.state_rewards\n",
    "        \n",
    "        # If horizon is random, don't provide the actual horizon\n",
    "        if self.random_horizon:\n",
    "            info['random_horizon'] = True\n",
    "            info['discount_factor'] = self.discount_factor\n",
    "            # Remove total_turns since it's unknown\n",
    "            info.pop('total_turns', None)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: The reward and exploration bonus obtained\n",
    "        \"\"\"\n",
    "        # Check if the episode has ended (for random horizon)\n",
    "        if self.random_horizon:\n",
    "            if self.current_turn >= self.actual_horizon:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        else:\n",
    "            if self.current_turn >= self.T:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        # Use the parent's step method\n",
    "        return super().step(action)\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the known transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Update each arm's state independently according to its transition matrix\n",
    "        for arm in range(2):\n",
    "            current_state = self.states[arm]\n",
    "            transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "            self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information including known transition matrices\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        info['transition_matrices'] = self.transition_matrices\n",
    "        info['state_rewards'] = self.state_rewards\n",
    "        return info\n",
    "\n",
    "\n",
    "class Problem2Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 2: Independent, Unknown Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Each arm follows an independent Markov process with unknown transition matrices.\n",
    "    The states are hidden from the agent, and the transition matrices are unknown.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 2 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        self.random_horizon = False\n",
    "        \n",
    "        # Generate more contrasting reward values for states\n",
    "        if state_rewards is None:\n",
    "            # Create more contrasting rewards between good and bad states\n",
    "            base_low = 0.05 + np.random.random() * 0.15  # Low reward between 0.05-0.2\n",
    "            base_high = 0.8 + np.random.random() * 0.15  # High reward between 0.8-0.95\n",
    "            self.state_rewards = [base_low, base_high]\n",
    "        else:\n",
    "            self.state_rewards = state_rewards\n",
    "        \n",
    "        # Generate transition matrices for both arms (unknown to the agent)\n",
    "        self.transition_matrices = [\n",
    "            self._generate_interesting_transition_matrix(state_count, arm_idx=i) for i in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _generate_interesting_transition_matrix(self, state_count: int, arm_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an interesting transition matrix with specific patterns.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "            arm_idx (int): Index of the arm (0 or 1) to generate different patterns\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((state_count, state_count))\n",
    "        \n",
    "        # Pattern type determined by arm index to create different strategic challenges\n",
    "        if arm_idx == 0:\n",
    "            # First arm has a tendency to deteriorate over time\n",
    "            # Higher probability to move to worse states from good states\n",
    "            for i in range(state_count):\n",
    "                # Base probability to stay in current state\n",
    "                stay_prob = 0.3 + np.random.random() * 0.3  # 0.3-0.6\n",
    "                matrix[i, i] = stay_prob\n",
    "                \n",
    "                remaining = 1.0 - stay_prob\n",
    "                \n",
    "                # Calculate transition probabilities based on current state\n",
    "                if i < state_count - 1:  # Not the worst state\n",
    "                    # Probability to move to worse states increases with state quality\n",
    "                    worse_bias = 0.3 + (i / (state_count - 1)) * 0.5  # 0.3-0.8 based on state\n",
    "                    better_bias = 1.0 - worse_bias\n",
    "                    \n",
    "                    # Calculate number of worse and better states\n",
    "                    worse_states = i\n",
    "                    better_states = state_count - i - 1\n",
    "                    \n",
    "                    # Distribute probabilities\n",
    "                    if worse_states > 0:\n",
    "                        worse_prob = remaining * worse_bias\n",
    "                        for j in range(worse_states):\n",
    "                            matrix[i, j] = worse_prob / worse_states\n",
    "                    \n",
    "                    if better_states > 0:\n",
    "                        better_prob = remaining * better_bias\n",
    "                        for j in range(i + 1, state_count):\n",
    "                            matrix[i, j] = better_prob / better_states\n",
    "                else:  # Worst state\n",
    "                    # From worst state, can only go to better states\n",
    "                    for j in range(i):\n",
    "                        matrix[i, j] = remaining / i\n",
    "        else:\n",
    "            # Second arm has alternating pattern - tends to swing between extremes\n",
    "            for i in range(state_count):\n",
    "                # Probability to stay\n",
    "                stay_prob = 0.2 + np.random.random() * 0.3  # 0.2-0.5\n",
    "                matrix[i, i] = stay_prob\n",
    "                \n",
    "                remaining = 1.0 - stay_prob\n",
    "                \n",
    "                # Find the most distant state\n",
    "                distant_idx = (i + state_count // 2) % state_count\n",
    "                \n",
    "                # Higher probability to jump to distant state\n",
    "                distant_prob = remaining * (0.6 + np.random.random() * 0.2)  # 60-80% of remaining\n",
    "                matrix[i, distant_idx] = distant_prob\n",
    "                \n",
    "                # Distribute remaining probability to other states\n",
    "                other_remaining = remaining - distant_prob\n",
    "                other_states = state_count - 2  # excluding current and distant\n",
    "                \n",
    "                if other_states > 0:\n",
    "                    for j in range(state_count):\n",
    "                        if j != i and j != distant_idx:\n",
    "                            matrix[i, j] = other_remaining / other_states\n",
    "        \n",
    "        # Normalize rows to ensure they sum to 1\n",
    "        for i in range(state_count):\n",
    "            row_sum = np.sum(matrix[i])\n",
    "            if row_sum > 0:\n",
    "                matrix[i] = matrix[i] / row_sum\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Update each arm's state independently according to its transition matrix\n",
    "        for arm in range(2):\n",
    "            current_state = self.states[arm]\n",
    "            transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "            self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information (transitions are unknown to the agent)\n",
    "        \"\"\"\n",
    "        # Same as base class, no transition information is provided\n",
    "        return super().get_visible_info()\n",
    "\n",
    "\n",
    "class Problem3Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 3: Dependent (Joint), Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Both arms follow a joint Markov process with known transition matrix.\n",
    "    The states are hidden from the agent, but the joint transition matrix is known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 3 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        self.random_horizon = False\n",
    "        \n",
    "        # Generate more contrasting reward values for states\n",
    "        if state_rewards is None:\n",
    "            # Create more contrasting rewards between good and bad states\n",
    "            base_low = 0.1 + np.random.random() * 0.15  # Low reward between 0.1-0.25\n",
    "            base_high = 0.8 + np.random.random() * 0.15  # High reward between 0.8-0.95\n",
    "            self.state_rewards = [base_low, base_high]\n",
    "        else:\n",
    "            self.state_rewards = state_rewards\n",
    "        \n",
    "        # Total number of joint states (state_count^2)\n",
    "        self.joint_state_count = state_count ** 2\n",
    "        \n",
    "        # Generate joint transition matrix with interesting patterns\n",
    "        self.joint_transition_matrix = self._generate_interesting_joint_transition_matrix()\n",
    "    \n",
    "    def _generate_interesting_joint_transition_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an interesting joint transition matrix with specific dependency patterns.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Joint transition matrix of shape (joint_state_count, joint_state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((self.joint_state_count, self.joint_state_count))\n",
    "        \n",
    "        # Create joint state transition patterns that show interesting dependencies\n",
    "        for i in range(self.joint_state_count):\n",
    "            # Convert joint state to individual states\n",
    "            state1, state2 = self._joint_state_to_individual(i)\n",
    "            \n",
    "            # Base probability to stay in current state\n",
    "            stay_prob = 0.2 + np.random.random() * 0.2  # 0.2-0.4\n",
    "            matrix[i, i] = stay_prob\n",
    "            \n",
    "            remaining = 1.0 - stay_prob\n",
    "            \n",
    "            # Define transition types based on current joint state\n",
    "            \n",
    "            # Type 1: Opposite movement - when arms are in same state, they tend to diverge\n",
    "            if state1 == state2:\n",
    "                # Find opposite states where one goes up and one goes down\n",
    "                opposite_states = []\n",
    "                for s1 in range(self.state_count):\n",
    "                    for s2 in range(self.state_count):\n",
    "                        # If one state improves and one worsens\n",
    "                        if (s1 > state1 and s2 < state2) or (s1 < state1 and s2 > state2):\n",
    "                            opposite_states.append(self._individual_to_joint_state([s1, s2]))\n",
    "                \n",
    "                if opposite_states:\n",
    "                    # 70% chance to move to opposite states\n",
    "                    opposite_prob = remaining * 0.7\n",
    "                    for j in opposite_states:\n",
    "                        matrix[i, j] = opposite_prob / len(opposite_states)\n",
    "                    \n",
    "                    # 30% distributed to other states\n",
    "                    other_prob = remaining * 0.3\n",
    "                    other_count = self.joint_state_count - 1 - len(opposite_states)\n",
    "                    if other_count > 0:\n",
    "                        for j in range(self.joint_state_count):\n",
    "                            if j != i and j not in opposite_states:\n",
    "                                matrix[i, j] = other_prob / other_count\n",
    "            \n",
    "            # Type 2: When first arm is good and second is bad, they tend to both become good\n",
    "            elif state1 > state2:\n",
    "                # Calculate better states for both arms\n",
    "                better_states = []\n",
    "                for s1 in range(state1, self.state_count):\n",
    "                    for s2 in range(state2 + 1, self.state_count):\n",
    "                        better_states.append(self._individual_to_joint_state([s1, s2]))\n",
    "                \n",
    "                if better_states:\n",
    "                    # 60% chance to improve both\n",
    "                    better_prob = remaining * 0.6\n",
    "                    for j in better_states:\n",
    "                        matrix[i, j] = better_prob / len(better_states)\n",
    "                    \n",
    "                    # 40% distributed to other states\n",
    "                    other_prob = remaining * 0.4\n",
    "                    other_count = self.joint_state_count - 1 - len(better_states)\n",
    "                    if other_count > 0:\n",
    "                        for j in range(self.joint_state_count):\n",
    "                            if j != i and j not in better_states:\n",
    "                                matrix[i, j] = other_prob / other_count\n",
    "            \n",
    "            # Type 3: When second arm is good and first is bad, they tend to both become bad\n",
    "            elif state1 < state2:\n",
    "                # Calculate worse states for both arms\n",
    "                worse_states = []\n",
    "                for s1 in range(0, state1):\n",
    "                    for s2 in range(0, state2):\n",
    "                        worse_states.append(self._individual_to_joint_state([s1, s2]))\n",
    "                \n",
    "                if worse_states:\n",
    "                    # 60% chance to both worsen\n",
    "                    worse_prob = remaining * 0.6\n",
    "                    for j in worse_states:\n",
    "                        matrix[i, j] = worse_prob / len(worse_states)\n",
    "                    \n",
    "                    # 40% distributed to other states\n",
    "                    other_prob = remaining * 0.4\n",
    "                    other_count = self.joint_state_count - 1 - len(worse_states)\n",
    "                    if other_count > 0:\n",
    "                        for j in range(self.joint_state_count):\n",
    "                            if j != i and j not in worse_states:\n",
    "                                matrix[i, j] = other_prob / other_count\n",
    "        \n",
    "        # Normalize rows to ensure they sum to 1\n",
    "        for i in range(self.joint_state_count):\n",
    "            row_sum = np.sum(matrix[i])\n",
    "            if row_sum > 0:\n",
    "                matrix[i] = matrix[i] / row_sum\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _joint_state_to_individual(self, joint_state: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a joint state index to individual arm states.\n",
    "        \n",
    "        Args:\n",
    "            joint_state (int): Joint state index\n",
    "        \n",
    "        Returns:\n",
    "            List[int]: Individual states [state1, state2]\n",
    "        \"\"\"\n",
    "        state1 = joint_state // self.state_count\n",
    "        state2 = joint_state % self.state_count\n",
    "        return [state1, state2]\n",
    "    \n",
    "    def _individual_to_joint_state(self, states: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convert individual arm states to a joint state index.\n",
    "        \n",
    "        Args:\n",
    "            states (List[int]): Individual states [state1, state2]\n",
    "        \n",
    "        Returns:\n",
    "            int: Joint state index\n",
    "        \"\"\"\n",
    "        return states[0] * self.state_count + states[1]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current joint state and the joint transition matrix.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Convert individual states to joint state\n",
    "        current_joint_state = self._individual_to_joint_state(self.states)\n",
    "        \n",
    "        # Get transition probabilities for the current joint state\n",
    "        transition_probabilities = self.joint_transition_matrix[current_joint_state]\n",
    "        \n",
    "        # Sample next joint state\n",
    "        next_joint_state = np.random.choice(self.joint_state_count, p=transition_probabilities)\n",
    "        \n",
    "        # Convert back to individual states\n",
    "        self.states = self._joint_state_to_individual(next_joint_state)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information including known joint transition matrix\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        info['joint_transition_matrix'] = self.joint_transition_matrix\n",
    "        info['state_count'] = self.state_count\n",
    "        info['state_rewards'] = self.state_rewards\n",
    "        return info\n",
    "\n",
    "\n",
    "class Problem4Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 4: Partially Observable / Possibly Unknown / Possibly Random T\n",
    "    \n",
    "    This is the most general case: states are hidden, transitions can be known or unknown,\n",
    "    and the horizon can be random or infinite.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None,\n",
    "                 known_transitions: bool = False, random_horizon: bool = False,\n",
    "                 joint_dynamics: bool = False, discount_factor: float = 0.95):\n",
    "        \"\"\"\n",
    "        Initialize Problem 4 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns or expected turns if random_horizon is True\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "            known_transitions (bool): Whether transitions are known to the agent\n",
    "            random_horizon (bool): Whether the time horizon is random\n",
    "            joint_dynamics (bool): Whether arms have joint dynamics or independent dynamics\n",
    "            discount_factor (float): Discount factor for infinite horizon (if random_horizon is True)\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        \n",
    "        # Generate more contrasting reward values for states\n",
    "        if state_rewards is None:\n",
    "            # Create significantly different rewards for different states\n",
    "            if state_count == 2:\n",
    "                # For binary states, create higher contrast\n",
    "                base_low = 0.05 + np.random.random() * 0.1  # Low reward between 0.05-0.15\n",
    "                base_high = 0.85 + np.random.random() * 0.1  # High reward between 0.85-0.95\n",
    "                self.state_rewards = [base_low, base_high]\n",
    "            else:\n",
    "                # For multiple states, create a progression with some randomization\n",
    "                self.state_rewards = []\n",
    "                for i in range(state_count):\n",
    "                    # Base reward increases with state index, with some noise\n",
    "                    base = 0.05 + (0.9 / (state_count - 1)) * i  # Linear progression from 0.05 to 0.95\n",
    "                    noise = np.random.random() * 0.1 - 0.05  # ±0.05 noise\n",
    "                    self.state_rewards.append(min(max(base + noise, 0.01), 0.99))  # Keep in [0.01, 0.99]\n",
    "        else:\n",
    "            self.state_rewards = state_rewards\n",
    "        \n",
    "        self.known_transitions = known_transitions\n",
    "        self.random_horizon = random_horizon\n",
    "        self.joint_dynamics = joint_dynamics\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # Generate transition matrices with interesting patterns\n",
    "        if joint_dynamics:\n",
    "            self.joint_state_count = state_count ** 2\n",
    "            self.joint_transition_matrix = self._generate_interesting_joint_transition_matrix()\n",
    "            self.transition_matrices = None\n",
    "        else:\n",
    "            self.transition_matrices = [\n",
    "                self._generate_interesting_transition_matrix(state_count, arm_idx=i) for i in range(2)\n",
    "            ]\n",
    "            self.joint_transition_matrix = None\n",
    "        \n",
    "        # If using a random horizon, generate the actual horizon\n",
    "        if random_horizon:\n",
    "            # Generate a geometric random variable with mean T\n",
    "            p = 1.0 / T\n",
    "            self.actual_horizon = np.random.geometric(p)\n",
    "        else:\n",
    "            self.actual_horizon = T\n",
    "            \n",
    "        # Initialize states explicitly (this is the key fix)\n",
    "        self._initialize_states()\n",
    "    \n",
    "    def _generate_interesting_transition_matrix(self, state_count: int, arm_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an interesting transition matrix for independent arm dynamics.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "            arm_idx (int): Index of the arm (0 or 1) to generate different patterns\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((state_count, state_count))\n",
    "        \n",
    "        # Create different transition patterns based on arm index\n",
    "        if arm_idx == 0:\n",
    "            # First arm has more predictable, structured transitions\n",
    "            # Higher probability to move in specific directions based on state\n",
    "            for i in range(state_count):\n",
    "                # Probability to stay in current state\n",
    "                position_factor = i / max(1, state_count - 1)  # 0 to 1 based on position\n",
    "                \n",
    "                # Better states are more stable, worse states less stable\n",
    "                stay_prob = 0.2 + position_factor * 0.5  # 0.2-0.7 based on state quality\n",
    "                matrix[i, i] = stay_prob\n",
    "                \n",
    "                remaining = 1.0 - stay_prob\n",
    "                \n",
    "                # Probability to move to better states decreases in better states\n",
    "                # Better states can more easily fall to worse states\n",
    "                if i < state_count - 1:  # Not in best state\n",
    "                    better_prob = remaining * (0.8 - position_factor * 0.6)  # 0.8-0.2 based on state\n",
    "                    better_states = state_count - i - 1\n",
    "                    \n",
    "                    for j in range(i + 1, state_count):\n",
    "                        if better_states > 0:\n",
    "                            # Higher probability to adjacent states, less to far states\n",
    "                            dist_factor = 1 - (j - i - 1) / better_states\n",
    "                            matrix[i, j] = better_prob * dist_factor / sum(1 - (k - i - 1) / better_states for k in range(i + 1, state_count))\n",
    "                \n",
    "                # Probability to move to worse states increases in better states\n",
    "                if i > 0:  # Not in worst state\n",
    "                    worse_prob = remaining * (0.2 + position_factor * 0.6)  # 0.2-0.8 based on state\n",
    "                    worse_states = i\n",
    "                    \n",
    "                    for j in range(i):\n",
    "                        if worse_states > 0:\n",
    "                            # Higher probability to adjacent states, less to far states\n",
    "                            dist_factor = 1 - (i - j - 1) / worse_states\n",
    "                            matrix[i, j] = worse_prob * dist_factor / sum(1 - (i - k - 1) / worse_states for k in range(i))\n",
    "        else:\n",
    "            # Second arm has more volatile but cyclical pattern\n",
    "            # Creates a wave-like movement pattern through states\n",
    "            for i in range(state_count):\n",
    "                # Lower probability to stay in any state\n",
    "                stay_prob = 0.1 + np.random.random() * 0.2  # 0.1-0.3\n",
    "                matrix[i, i] = stay_prob\n",
    "                \n",
    "                remaining = 1.0 - stay_prob\n",
    "                \n",
    "                # Calculate distance to other states (in a circular sense)\n",
    "                distances = {}\n",
    "                for j in range(state_count):\n",
    "                    if j != i:\n",
    "                        # Calculate circular distance (smaller of clockwise and counterclockwise)\n",
    "                        clockwise = (j - i) % state_count\n",
    "                        counterclockwise = (i - j) % state_count\n",
    "                        distances[j] = min(clockwise, counterclockwise)\n",
    "                \n",
    "                # Assign higher probabilities to states at specific distances\n",
    "                # Create a wave pattern where probability peaks at certain distances\n",
    "                total_weight = 0\n",
    "                weights = {}\n",
    "                for j, distance in distances.items():\n",
    "                    # Create a sinusoidal weighting based on distance\n",
    "                    # Peak at distance = 1 and distance = state_count//2\n",
    "                    if distance == 1:\n",
    "                        weights[j] = 0.8 + np.random.random() * 0.2  # High probability to adjacent\n",
    "                    elif distance == state_count // 2:\n",
    "                        weights[j] = 0.5 + np.random.random() * 0.3  # Medium probability to opposite\n",
    "                    else:\n",
    "                        # Gradually decrease for other distances\n",
    "                        weights[j] = 0.1 + (0.3 * np.sin(np.pi * distance / state_count))\n",
    "                    total_weight += weights[j]\n",
    "                \n",
    "                # Normalize weights and assign to matrix\n",
    "                for j, weight in weights.items():\n",
    "                    matrix[i, j] = remaining * weight / total_weight\n",
    "        \n",
    "        # Normalize rows to ensure they sum to 1\n",
    "        for i in range(state_count):\n",
    "            row_sum = np.sum(matrix[i])\n",
    "            if row_sum > 0:\n",
    "                matrix[i] = matrix[i] / row_sum\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def _generate_interesting_joint_transition_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an interesting joint transition matrix with complex dependency patterns.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Joint transition matrix of shape (joint_state_count, joint_state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.zeros((self.joint_state_count, self.joint_state_count))\n",
    "        \n",
    "        # Create patterns showing strong dependencies between arms\n",
    "        for i in range(self.joint_state_count):\n",
    "            # Convert joint state to individual states\n",
    "            state1, state2 = self._joint_state_to_individual(i)\n",
    "            \n",
    "            # Base probability to stay in current state\n",
    "            # Joint states where both arms are in similar states are more stable\n",
    "            similarity = 1 - abs(state1 - state2) / (self.state_count - 1)\n",
    "            stay_prob = 0.1 + similarity * 0.4  # 0.1-0.5 based on similarity\n",
    "            matrix[i, i] = stay_prob\n",
    "            \n",
    "            remaining = 1.0 - stay_prob\n",
    "            \n",
    "            # Pattern 1: States tend to converge or diverge based on current relationship\n",
    "            if state1 == state2:  # Same state for both arms\n",
    "                # When arms are in same state, create a diverging pattern\n",
    "                # Higher probability to move to states where one arm improves, one worsens\n",
    "                diverge_states = []\n",
    "                diverge_weights = []\n",
    "                \n",
    "                for s1 in range(self.state_count):\n",
    "                    for s2 in range(self.state_count):\n",
    "                        if (s1 != s2) and (s1 != state1 or s2 != state2):\n",
    "                            # Calculate how much this represents a divergence\n",
    "                            divergence = abs(s1 - s2) / (self.state_count - 1)\n",
    "                            if divergence > 0:\n",
    "                                joint_state = self._individual_to_joint_state([s1, s2])\n",
    "                                diverge_states.append(joint_state)\n",
    "                                diverge_weights.append(divergence)\n",
    "                \n",
    "                if diverge_states:\n",
    "                    # Normalize weights\n",
    "                    total_weight = sum(diverge_weights)\n",
    "                    if total_weight > 0:\n",
    "                        normalized_weights = [w / total_weight for w in diverge_weights]\n",
    "                        diverge_prob = remaining * 0.7  # 70% of remaining probability\n",
    "                        \n",
    "                        for idx, joint_state in enumerate(diverge_states):\n",
    "                            matrix[i, joint_state] = diverge_prob * normalized_weights[idx]\n",
    "                    \n",
    "                    # Distribute remaining probability\n",
    "                    other_states_prob = remaining * 0.3\n",
    "                    other_states_count = self.joint_state_count - 1 - len(diverge_states)\n",
    "                    if other_states_count > 0:\n",
    "                        per_state_prob = other_states_prob / other_states_count\n",
    "                        for j in range(self.joint_state_count):\n",
    "                            if j != i and j not in diverge_states:\n",
    "                                matrix[i, j] = per_state_prob\n",
    "            \n",
    "            else:  # Different states for arms\n",
    "                # When arms are in different states, create a converging pattern\n",
    "                # Higher probability to move to states where arms become more similar\n",
    "                converge_states = []\n",
    "                converge_weights = []\n",
    "                \n",
    "                for s1 in range(self.state_count):\n",
    "                    for s2 in range(self.state_count):\n",
    "                        if (s1 != state1 or s2 != state2):\n",
    "                            # Calculate how much this represents a convergence\n",
    "                            current_diff = abs(state1 - state2)\n",
    "                            new_diff = abs(s1 - s2)\n",
    "                            convergence = max(0, current_diff - new_diff) / current_diff if current_diff > 0 else 0\n",
    "                            \n",
    "                            if convergence > 0:\n",
    "                                joint_state = self._individual_to_joint_state([s1, s2])\n",
    "                                converge_states.append(joint_state)\n",
    "                                converge_weights.append(convergence)\n",
    "                \n",
    "                if converge_states:\n",
    "                    # Normalize weights\n",
    "                    total_weight = sum(converge_weights)\n",
    "                    if total_weight > 0:\n",
    "                        normalized_weights = [w / total_weight for w in converge_weights]\n",
    "                        converge_prob = remaining * 0.6  # 60% of remaining probability\n",
    "                        \n",
    "                        for idx, joint_state in enumerate(converge_states):\n",
    "                            matrix[i, joint_state] = converge_prob * normalized_weights[idx]\n",
    "                    \n",
    "                    # Distribute remaining probability\n",
    "                    other_states_prob = remaining * 0.4\n",
    "                    other_states_count = self.joint_state_count - 1 - len(converge_states)\n",
    "                    if other_states_count > 0:\n",
    "                        per_state_prob = other_states_prob / other_states_count\n",
    "                        for j in range(self.joint_state_count):\n",
    "                            if j != i and j not in converge_states:\n",
    "                                matrix[i, j] = per_state_prob\n",
    "        \n",
    "        # Normalize rows to ensure they sum to 1\n",
    "        for i in range(self.joint_state_count):\n",
    "            row_sum = np.sum(matrix[i])\n",
    "            if row_sum > 0:\n",
    "                matrix[i] = matrix[i] / row_sum\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "        \n",
    "    def _joint_state_to_individual(self, joint_state: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a joint state index to individual arm states.\n",
    "        \n",
    "        Args:\n",
    "            joint_state (int): Joint state index\n",
    "        \n",
    "        Returns:\n",
    "            List[int]: Individual states [state1, state2]\n",
    "        \"\"\"\n",
    "        state1 = joint_state // self.state_count\n",
    "        state2 = joint_state % self.state_count\n",
    "        return [state1, state2]\n",
    "    \n",
    "    def _individual_to_joint_state(self, states: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convert individual arm states to a joint state index.\n",
    "        \n",
    "        Args:\n",
    "            states (List[int]): Individual states [state1, state2]\n",
    "        \n",
    "        Returns:\n",
    "            int: Joint state index\n",
    "        \"\"\"\n",
    "        return states[0] * self.state_count + states[1]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and action.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        if self.joint_dynamics:\n",
    "            # Convert individual states to joint state\n",
    "            current_joint_state = self._individual_to_joint_state(self.states)\n",
    "            \n",
    "            # Get transition probabilities for the current joint state\n",
    "            transition_probabilities = self.joint_transition_matrix[current_joint_state]\n",
    "            \n",
    "            # Sample next joint state\n",
    "            next_joint_state = np.random.choice(self.joint_state_count, p=transition_probabilities)\n",
    "            \n",
    "            # Convert back to individual states\n",
    "            self.states = self._joint_state_to_individual(next_joint_state)\n",
    "        else:\n",
    "            # Update each arm's state independently\n",
    "            for arm in range(2):\n",
    "                current_state = self.states[arm]\n",
    "                transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "                self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        \n",
    "        # Add transition information if known\n",
    "        if self.known_transitions:\n",
    "            if self.joint_dynamics:\n",
    "                info['joint_transition_matrix'] = self.joint_transition_matrix\n",
    "                info['joint_dynamics'] = True\n",
    "            else:\n",
    "                info['transition_matrices'] = self.transition_matrices\n",
    "                info['joint_dynamics'] = False\n",
    "            \n",
    "            info['state_rewards'] = self.state_rewards\n",
    "        \n",
    "        # If horizon is random, don't provide the actual horizon\n",
    "        if self.random_horizon:\n",
    "            info['random_horizon'] = True\n",
    "            info['discount_factor'] = self.discount_factor\n",
    "            # Remove total_turns since it's unknown\n",
    "            info.pop('total_turns', None)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: The reward and exploration bonus obtained\n",
    "        \"\"\"\n",
    "        # Check if the episode has ended (for random horizon)\n",
    "        if self.random_horizon:\n",
    "            if self.current_turn >= self.actual_horizon:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        else:\n",
    "            if self.current_turn >= self.T:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        # Use the parent's step method\n",
    "        return super().step(action)\n",
    "\n",
    "# ================ EXPERIMENT RUNNER ================\n",
    "\n",
    "def run_experiment(\n",
    "    agent_func: Callable,\n",
    "    env_class: Callable,\n",
    "    env_params: Dict = None,\n",
    "    n_games: int = 50,\n",
    "    default_turns: int = 100,\n",
    "    random_turns: bool = False,\n",
    "    verbose: bool = True,\n",
    "    pbar: Optional[tqdm] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run an experiment with the given agent on a specific Markov bandit environment.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        env_class (Callable): The environment class to use\n",
    "        env_params (Dict, optional): Additional parameters for the environment\n",
    "        n_games (int): Number of games to play per environment\n",
    "        default_turns (int): Default number of turns per game (used when random_turns=False)\n",
    "        random_turns (bool): Whether to use random number of turns\n",
    "        verbose (bool): Whether to show progress bar\n",
    "        pbar (tqdm, optional): External progress bar to update instead of creating a new one\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed each time\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    # Default environment parameters\n",
    "    if env_params is None:\n",
    "        env_params = {}\n",
    "    \n",
    "    # Create environment\n",
    "    env = env_class(**env_params)\n",
    "    env_name = env_class.__name__\n",
    "    \n",
    "    results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": [],\n",
    "        \"exploration_bonus\": []\n",
    "    }\n",
    "    \n",
    "    # Create turn counts in advance for reproducibility\n",
    "    all_turn_counts = []\n",
    "    if random_turns:\n",
    "        for _ in range(n_games):\n",
    "            all_turn_counts.append(np.random.randint(1, 301))\n",
    "    else:\n",
    "        all_turn_counts = [default_turns] * n_games\n",
    "    \n",
    "    # Use tqdm for progress if verbose and no external pbar\n",
    "    if verbose and pbar is None:\n",
    "        game_iterator = tqdm(range(n_games), desc=f\"Running {env_name}\")\n",
    "    elif pbar is not None:\n",
    "        # Use external progress bar without creating a range iterator\n",
    "        game_iterator = range(n_games)\n",
    "    else:\n",
    "        game_iterator = range(n_games)\n",
    "    \n",
    "    for game_idx in game_iterator:\n",
    "        # Get pre-generated turn count for this game\n",
    "        T = all_turn_counts[game_idx]\n",
    "        \n",
    "        env.reset(T)\n",
    "        \n",
    "        total_reward = 0\n",
    "        total_exploration_bonus = 0\n",
    "        optimal_actions = 0\n",
    "        \n",
    "        # Start the game\n",
    "        game_over = False\n",
    "        while not game_over:\n",
    "            try:\n",
    "                # Get visible information for the agent\n",
    "                visible_info = env.get_visible_info()\n",
    "                \n",
    "                # Get action from agent\n",
    "                action = agent_func(visible_info)\n",
    "                \n",
    "                # Take step in environment\n",
    "                reward, exploration_bonus = env.step(action)\n",
    "                total_reward += reward\n",
    "                total_exploration_bonus += exploration_bonus\n",
    "                \n",
    "                # Count optimal actions (not easily determined in hidden state environments)\n",
    "                # For simplicity, we'll consider an action optimal if it gave a reward\n",
    "                if reward > 0:\n",
    "                    optimal_actions += 1\n",
    "                \n",
    "            except ValueError as e:\n",
    "                # Game is over\n",
    "                game_over = True\n",
    "        \n",
    "        # For Problem 4 with random horizon, record the actual horizon used\n",
    "        if hasattr(env, 'actual_horizon'):\n",
    "            turns_played = env.actual_horizon\n",
    "        else:\n",
    "            turns_played = T\n",
    "        \n",
    "        # Calculate regret (difficult to define perfectly for Markov bandits)\n",
    "        # We'll use a simplified approach: regret = max possible reward - actual reward\n",
    "        # where max possible reward assumes perfect knowledge of states\n",
    "        # This is an approximation and not the true regret\n",
    "        regret = turns_played - total_reward\n",
    "        \n",
    "        # Store results\n",
    "        results[\"environment\"].append(env_name)\n",
    "        results[\"game\"].append(game_idx)\n",
    "        results[\"total_reward\"].append(total_reward)\n",
    "        results[\"average_reward\"].append(total_reward / turns_played)\n",
    "        results[\"turns\"].append(turns_played)\n",
    "        results[\"actions\"].append(env.history['actions'])\n",
    "        results[\"optimal_actions\"].append(optimal_actions / turns_played * 100)  # percentage\n",
    "        results[\"regret\"].append(regret)\n",
    "        results[\"exploration_bonus\"].append(total_exploration_bonus)\n",
    "        \n",
    "        # Update external progress bar if provided\n",
    "        if pbar is not None:\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_standard_experiment(agent_func: Callable, problem_number: int, n_experiments: int = 100, \n",
    "                           fixed_turns: bool = True, exploration_bonus: float = 0.1) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a standardized experiment for a specific agent on a specific problem.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        problem_number (int): Problem number (1-4)\n",
    "        n_experiments (int): Number of experiments to run (default 100)\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed for this experiment\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    # Set environment class based on problem number\n",
    "    env_classes = {\n",
    "        1: Problem1Environment,\n",
    "        2: Problem2Environment,\n",
    "        3: Problem3Environment,\n",
    "        4: Problem4Environment\n",
    "    }\n",
    "    \n",
    "    if problem_number not in env_classes:\n",
    "        raise ValueError(f\"Invalid problem number: {problem_number}. Must be 1-4.\")\n",
    "    \n",
    "    env_class = env_classes[problem_number]\n",
    "    \n",
    "    # Set environment parameters\n",
    "    env_params = {\n",
    "        'exploration_bonus': exploration_bonus,\n",
    "        'state_count': 2\n",
    "    }\n",
    "    \n",
    "    # For Problem 4, add additional parameters\n",
    "    if problem_number == 4:\n",
    "        env_params.update({\n",
    "            'known_transitions': True,  # Set to False to test with unknown transitions\n",
    "            'random_horizon': not fixed_turns,  # Use random horizon if not fixed_turns\n",
    "            'joint_dynamics': False,  # Set to True to test with joint dynamics\n",
    "            'discount_factor': 0.95  # Used for random/infinite horizon\n",
    "        })\n",
    "    \n",
    "    # Run the experiment\n",
    "    return run_experiment(\n",
    "        agent_func=agent_func,\n",
    "        env_class=env_class,\n",
    "        env_params=env_params,\n",
    "        n_games=n_experiments,\n",
    "        default_turns=100,\n",
    "        random_turns=not fixed_turns,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "# ================ VISUALIZATION FUNCTIONS ================\n",
    "\n",
    "def plot_rewards_by_environment(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot average rewards for the environment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df)\n",
    "    plt.title('Distribution of Average Rewards')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_optimal_actions(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot percentage of optimal actions.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Optimal Actions (%)': results['optimal_actions']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df)\n",
    "    plt.title('Percentage of Optimal Actions')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_regret(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot regret.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Regret': results['regret']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df)\n",
    "    plt.title('Distribution of Regret')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_action_history(results: Dict, game_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot action history for a specific game.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        game_idx (int): Index of the game to plot\n",
    "    \"\"\"\n",
    "    env_name = results['environment'][game_idx]\n",
    "    actions = results['actions'][game_idx]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(actions, 'o-', markersize=4)\n",
    "    plt.yticks([0, 1], ['Arm 1', 'Arm 2'])\n",
    "    plt.title(f'Action History for {env_name} - Game {results[\"game\"][game_idx]}')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Action')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_reward_over_time(results: Dict, n_games: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Plot cumulative reward over time for selected games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        n_games (int): Number of games to plot\n",
    "    \"\"\"\n",
    "    # Select game indices to plot\n",
    "    env_names = list(set(results['environment']))\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for env_name in env_names:\n",
    "        env_game_indices = [i for i, e in enumerate(results['environment']) if e == env_name]\n",
    "        \n",
    "        # Select a subset of games for this environment\n",
    "        selected_indices = env_game_indices[:n_games]\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            actions = results['actions'][idx]\n",
    "            turns = results['turns'][idx]\n",
    "            \n",
    "            # Reconstruct cumulative rewards\n",
    "            cum_rewards = np.cumsum([results['total_reward'][idx] / turns] * turns)\n",
    "            \n",
    "            plt.plot(cum_rewards, alpha=0.7, label=f\"{env_name} - Game {results['game'][idx]}\")\n",
    "    \n",
    "    plt.title('Cumulative Reward Over Time')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_statistical_summary(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive statistical summary from experiment results.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistical summary\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Total Reward': results['total_reward'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Turns': results['turns'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i],\n",
    "            'Exploration Bonus': results['exploration_bonus'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Group by environment and calculate statistics\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std', 'min', 'max'],\n",
    "        'Exploration Bonus': ['mean', 'std', 'min', 'max'],\n",
    "        'Turns': ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_performance_metrics(results: Dict, title_prefix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Plot comprehensive performance metrics for the agent.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        title_prefix (str): Prefix for plot titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret'],\n",
    "        'Turns': results['turns'],\n",
    "        'Exploration Bonus': results['exploration_bonus']\n",
    "    })\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot average reward\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'{title_prefix}Average Reward by Environment')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal action percentage\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'{title_prefix}Optimal Actions (%) by Environment')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regret\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'{title_prefix}Regret by Environment')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot exploration bonus\n",
    "    sns.boxplot(x='Environment', y='Exploration Bonus', data=df, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'{title_prefix}Exploration Bonus by Environment')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_simplified_results(results: Dict, agent_name: str = \"Agent\") -> None:\n",
    "    \"\"\"\n",
    "    Simplified visualization showing only aggregate results across all games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        agent_name (str): Name of the agent for titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i],\n",
    "            'Turns': results['turns'][i],\n",
    "            'Exploration Bonus': results['exploration_bonus'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate and display statistical summary\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std'],\n",
    "        'Exploration Bonus': ['mean', 'std'],\n",
    "        'Turns': ['mean', 'std', 'min', 'max', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    # Print agent name and summary statistics\n",
    "    print(f\"\\n==== {agent_name} Summary Statistics ====\")\n",
    "    summary_display = summary.round(3)  # Round to 3 decimal places for cleaner display\n",
    "    print(summary_display)\n",
    "    \n",
    "    # Create a figure with key metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Environment', y='Average Reward', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Average Reward by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Environment', y='Optimal Actions (%)', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Optimal Actions (%) by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Environment', y='Regret', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Regret by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Exploration Bonus\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Environment', y='Exploration Bonus', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Exploration Bonus by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{agent_name} Performance\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_results(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the results of the experiment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    # Convert results to pandas DataFrame for easier analysis\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Total Reward': results['total_reward'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Turns': results['turns'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret'],\n",
    "        'Exploration Bonus': results['exploration_bonus']\n",
    "    })\n",
    "    \n",
    "    # Summary statistics by environment\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std'],\n",
    "        'Average Reward': ['mean', 'std'],\n",
    "        'Optimal Actions (%)': ['mean', 'std'],\n",
    "        'Regret': ['mean', 'std'],\n",
    "        'Exploration Bonus': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    print(\"===== Summary Statistics =====\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_rewards_by_environment(results)\n",
    "    plot_optimal_actions(results)\n",
    "    plot_regret(results)\n",
    "    \n",
    "    # Plot action history for a game\n",
    "    if len(results['environment']) > 0:\n",
    "        plot_action_history(results, 0)\n",
    "    \n",
    "    # Plot reward over time for a subset of games\n",
    "    plot_reward_over_time(results, n_games=2)\n",
    "\n",
    "\n",
    "# ================ EXPLORATION BONUS ANALYSIS FUNCTIONS ================\n",
    "\n",
    "def visualize_bonus_comparison(results_by_bonus: Dict, title_prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Visualize how performance metrics change with different exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        results_by_bonus (Dict): Dictionary mapping bonus values to experiment results\n",
    "        title_prefix (str): Prefix for plot titles\n",
    "    \"\"\"\n",
    "    # Extract metrics for each bonus value\n",
    "    bonus_values = []\n",
    "    avg_rewards = []\n",
    "    optimal_actions = []\n",
    "    regrets = []\n",
    "    exploration_bonuses = []\n",
    "    \n",
    "    for bonus, results in sorted(results_by_bonus.items()):\n",
    "        bonus_values.append(bonus)\n",
    "        \n",
    "        # Compute mean metrics\n",
    "        df = pd.DataFrame({\n",
    "            'Average Reward': results['average_reward'],\n",
    "            'Optimal Actions (%)': results['optimal_actions'],\n",
    "            'Regret': results['regret'],\n",
    "            'Exploration Bonus': results['exploration_bonus']\n",
    "        })\n",
    "        \n",
    "        avg_rewards.append(df['Average Reward'].mean())\n",
    "        optimal_actions.append(df['Optimal Actions (%)'].mean())\n",
    "        regrets.append(df['Regret'].mean())\n",
    "        exploration_bonuses.append(df['Exploration Bonus'].mean())\n",
    "    \n",
    "    # Create figure with multiple plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Plot average reward vs. exploration bonus\n",
    "    axes[0, 0].plot(bonus_values, avg_rewards, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "    axes[0, 0].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[0, 0].set_ylabel('Average Reward')\n",
    "    axes[0, 0].set_title(f'{title_prefix}: Average Reward vs. Exploration Bonus')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal actions vs. exploration bonus\n",
    "    axes[0, 1].plot(bonus_values, optimal_actions, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[0, 1].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[0, 1].set_ylabel('Optimal Actions (%)')\n",
    "    axes[0, 1].set_title(f'{title_prefix}: Optimal Actions vs. Exploration Bonus')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regret vs. exploration bonus\n",
    "    axes[1, 0].plot(bonus_values, regrets, 'o-', linewidth=2, markersize=8, color='red')\n",
    "    axes[1, 0].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[1, 0].set_ylabel('Regret')\n",
    "    axes[1, 0].set_title(f'{title_prefix}: Regret vs. Exploration Bonus')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot total exploration bonus received vs. parameter\n",
    "    axes[1, 1].plot(bonus_values, exploration_bonuses, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "    axes[1, 1].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[1, 1].set_ylabel('Total Exploration Bonus')\n",
    "    axes[1, 1].set_title(f'{title_prefix}: Accumulated Exploration Bonus')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{title_prefix}: Impact of Exploration Bonus Parameter\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create boxplots for each metric\n",
    "    metrics = [\n",
    "        ('Average Reward', 'blue'), \n",
    "        ('Optimal Actions (%)', 'green'), \n",
    "        ('Regret', 'red')\n",
    "    ]\n",
    "    \n",
    "    for metric, color in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Prepare data for boxplot\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for bonus, results in sorted(results_by_bonus.items()):\n",
    "            if metric == 'Average Reward':\n",
    "                data.append(results['average_reward'])\n",
    "            elif metric == 'Optimal Actions (%)':\n",
    "                data.append(results['optimal_actions'])\n",
    "            else:  # Regret\n",
    "                data.append(results['regret'])\n",
    "            \n",
    "            labels.append(f'β={bonus}')\n",
    "        \n",
    "        # Create boxplot\n",
    "        boxplot = plt.boxplot(data, labels=labels, patch_artist=True)\n",
    "        \n",
    "        # Color boxes\n",
    "        for box in boxplot['boxes']:\n",
    "            box.set(facecolor=color, alpha=0.6)\n",
    "        \n",
    "        plt.title(f'{title_prefix}: {metric} Distribution by Exploration Bonus')\n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel(metric)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    summary_data = []\n",
    "    for i, bonus in enumerate(bonus_values):\n",
    "        summary_data.append({\n",
    "            'Exploration Bonus (β)': bonus,\n",
    "            'Average Reward': avg_rewards[i],\n",
    "            'Optimal Actions (%)': optimal_actions[i],\n",
    "            'Regret': regrets[i],\n",
    "            'Total Exploration Bonus': exploration_bonuses[i]\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(f\"\\n{title_prefix}: Summary Table of Metrics by Exploration Bonus\")\n",
    "    print(summary_df.round(4))\n",
    "\n",
    "\n",
    "def visualize_all_problems_bonus_comparison(all_results: Dict):\n",
    "    \"\"\"\n",
    "    Visualize how exploration bonus affects performance across all problems.\n",
    "    \n",
    "    Args:\n",
    "        all_results (Dict): Nested dictionary: problem -> bonus -> results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    problem_names = []\n",
    "    bonus_values = []\n",
    "    avg_rewards = []\n",
    "    optimal_actions = []\n",
    "    regrets = []\n",
    "    \n",
    "    for problem_name, problem_results in all_results.items():\n",
    "        for bonus, results in sorted(problem_results.items()):\n",
    "            problem_names.append(problem_name)\n",
    "            bonus_values.append(bonus)\n",
    "            \n",
    "            # Compute mean metrics\n",
    "            df = pd.DataFrame({\n",
    "                'Average Reward': results['average_reward'],\n",
    "                'Optimal Actions (%)': results['optimal_actions'],\n",
    "                'Regret': results['regret']\n",
    "            })\n",
    "            \n",
    "            avg_rewards.append(df['Average Reward'].mean())\n",
    "            optimal_actions.append(df['Optimal Actions (%)'].mean())\n",
    "            regrets.append(df['Regret'].mean())\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Problem': problem_names,\n",
    "        'Exploration Bonus': bonus_values,\n",
    "        'Average Reward': avg_rewards,\n",
    "        'Optimal Actions (%)': optimal_actions,\n",
    "        'Regret': regrets\n",
    "    })\n",
    "    \n",
    "    # Create line plots showing each problem's performance across bonus values\n",
    "    metrics = ['Average Reward', 'Optimal Actions (%)', 'Regret']\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for metric, color in zip(metrics, colors):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for problem in sorted(set(problem_names)):\n",
    "            problem_df = plot_df[plot_df['Problem'] == problem]\n",
    "            plt.plot(\n",
    "                problem_df['Exploration Bonus'], \n",
    "                problem_df[metric],\n",
    "                'o-',\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                label=problem\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'Effect of Exploration Bonus on {metric} Across Problems')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create heatmaps showing the effect of bonus across problems\n",
    "    for metric in metrics:\n",
    "        # Reshape data for heatmap\n",
    "        pivot_df = plot_df.pivot_table(\n",
    "            index='Problem',\n",
    "            columns='Exploration Bonus',\n",
    "            values=metric\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(pivot_df, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "        plt.title(f'Heatmap of {metric} by Problem and Exploration Bonus')\n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel('Problem')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def evaluate_problem_with_varying_bonus(problem_number: int, \n",
    "                                      bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                      fixed_turns: bool = True,\n",
    "                                      n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments for a specific problem with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        problem_number (int): Problem number (1-4)\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    agent_funcs = {\n",
    "        1: problem1_agent,\n",
    "        2: problem2_agent,\n",
    "        3: problem3_agent,\n",
    "        4: problem4_agent\n",
    "    }\n",
    "    \n",
    "    if problem_number not in agent_funcs:\n",
    "        raise ValueError(f\"Invalid problem number: {problem_number}. Must be 1-4.\")\n",
    "    \n",
    "    agent_func = agent_funcs[problem_number]\n",
    "    \n",
    "    # Store results for each bonus value\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create a single progress bar for all bonus values\n",
    "    total_experiments = len(bonus_values) * n_experiments\n",
    "    with tqdm(total=total_experiments, desc=f\"Problem {problem_number} β Analysis\") as pbar:\n",
    "        for bonus in bonus_values:\n",
    "            # Create environment\n",
    "            env_class = [Problem1Environment, Problem2Environment, Problem3Environment, Problem4Environment][problem_number-1]\n",
    "            env_params = {\n",
    "                'exploration_bonus': bonus,\n",
    "                'state_count': 2\n",
    "            }\n",
    "            \n",
    "            # For Problem 4, add additional parameters\n",
    "            if problem_number == 4:\n",
    "                env_params.update({\n",
    "                    'known_transitions': True,\n",
    "                    'random_horizon': not fixed_turns,\n",
    "                    'joint_dynamics': False,\n",
    "                    'discount_factor': 0.95\n",
    "                })\n",
    "            \n",
    "            # Run without internal progress bar\n",
    "            results = run_experiment(\n",
    "                agent_func=agent_func,\n",
    "                env_class=env_class,\n",
    "                env_params=env_params,\n",
    "                n_games=n_experiments,\n",
    "                default_turns=100,\n",
    "                random_turns=not fixed_turns,\n",
    "                verbose=False,  # Disable internal progress bar\n",
    "                pbar=pbar      # Pass our progress bar\n",
    "            )\n",
    "            \n",
    "            all_results[bonus] = results\n",
    "    \n",
    "    # Visualize the comparative results\n",
    "    visualize_bonus_comparison(all_results, f\"Problem {problem_number}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def evaluate_problem1_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 1 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=1,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem2_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 2 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=2,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem3_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 3 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=3,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem4_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 4 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=4,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def run_all_agents_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                     fixed_turns: bool = True,\n",
    "                                     n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments for all problems with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all problems and bonus values\n",
    "    \"\"\"\n",
    "    # Store results for each problem\n",
    "    all_results = {}\n",
    "    \n",
    "    # Combined progress tracking for all problems\n",
    "    total_experiments = 4 * len(bonus_values) * n_experiments\n",
    "    with tqdm(total=total_experiments, desc=f\"Exploration Analysis\") as pbar:\n",
    "        for problem_number in range(1, 5):\n",
    "            # Get problem class\n",
    "            env_class = [Problem1Environment, Problem2Environment, Problem3Environment, Problem4Environment][problem_number-1]\n",
    "            agent_func = [problem1_agent, problem2_agent, problem3_agent, problem4_agent][problem_number-1]\n",
    "            \n",
    "            # Store results for each bonus value\n",
    "            problem_results = {}\n",
    "            \n",
    "            for bonus in bonus_values:\n",
    "                # Create environment\n",
    "                env_params = {\n",
    "                    'exploration_bonus': bonus,\n",
    "                    'state_count': 2\n",
    "                }\n",
    "                \n",
    "                # For Problem 4, add additional parameters\n",
    "                if problem_number == 4:\n",
    "                    env_params.update({\n",
    "                        'known_transitions': True,\n",
    "                        'random_horizon': not fixed_turns,\n",
    "                        'joint_dynamics': False,\n",
    "                        'discount_factor': 0.95\n",
    "                    })\n",
    "                \n",
    "                # Run without internal progress bar\n",
    "                results = run_experiment(\n",
    "                    agent_func=agent_func,\n",
    "                    env_class=env_class,\n",
    "                    env_params=env_params,\n",
    "                    n_games=n_experiments,\n",
    "                    default_turns=100,\n",
    "                    random_turns=not fixed_turns,\n",
    "                    verbose=False,  # Disable internal progress bar\n",
    "                    pbar=pbar      # Pass our progress bar\n",
    "                )\n",
    "                \n",
    "                problem_results[bonus] = results\n",
    "            \n",
    "            all_results[f\"problem{problem_number}\"] = problem_results\n",
    "            \n",
    "            # Visualize after each problem\n",
    "            print(f\"\\n==== Problem {problem_number} Exploration Analysis Results ====\")\n",
    "            visualize_bonus_comparison(problem_results, f\"Problem {problem_number}\")\n",
    "    \n",
    "    # Visualize comparative results across all problems\n",
    "    print(\"\\n==== Cross-Problem Exploration Analysis ====\")\n",
    "    visualize_all_problems_bonus_comparison(all_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def evaluate_problem1_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 1 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 1 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem1_agent,\n",
    "        problem_number=1,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 1 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem2_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 2 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 2 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem2_agent,\n",
    "        problem_number=2,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 2 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem3_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 3 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 3 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem3_agent,\n",
    "        problem_number=3,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 3 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem4_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 4 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 4 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem4_agent,\n",
    "        problem_number=4,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 4 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_all_agents(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standard evaluations for all agent types.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run for each agent\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Comprehensive results for all agents\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each agent type\n",
    "    print(\"\\n==== Running Problem 1 Agent ====\")\n",
    "    results[\"problem1\"] = evaluate_problem1_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 2 Agent ====\")\n",
    "    results[\"problem2\"] = evaluate_problem2_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 3 Agent ====\")\n",
    "    results[\"problem3\"] = evaluate_problem3_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 4 Agent ====\")\n",
    "    results[\"problem4\"] = evaluate_problem4_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    # Compare agents\n",
    "    print(\"\\n==== Agent Comparison ====\")\n",
    "    compare_all_agents(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_all_agents(all_results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Compare the performance of all agents.\n",
    "    \n",
    "    Args:\n",
    "        all_results (Dict): Results from run_all_agents\n",
    "    \"\"\"\n",
    "    # Prepare data for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Collect results from each agent\n",
    "    for agent_name, results in all_results.items():\n",
    "        # Calculate mean performance metrics for this agent\n",
    "        df = pd.DataFrame({\n",
    "            'Environment': results['environment'],\n",
    "            'Average Reward': results['average_reward'],\n",
    "            'Optimal Actions (%)': results['optimal_actions'],\n",
    "            'Regret': results['regret'],\n",
    "            'Exploration Bonus': results['exploration_bonus']\n",
    "        })\n",
    "        \n",
    "        # Get mean values\n",
    "        mean_reward = df['Average Reward'].mean()\n",
    "        mean_optimal = df['Optimal Actions (%)'].mean()\n",
    "        mean_regret = df['Regret'].mean()\n",
    "        mean_bonus = df['Exploration Bonus'].mean()\n",
    "        \n",
    "        # Add to comparison data\n",
    "        comparison_data.append({\n",
    "            'Agent': agent_name,\n",
    "            'Average Reward': mean_reward,\n",
    "            'Optimal Actions (%)': mean_optimal,\n",
    "            'Regret': mean_regret,\n",
    "            'Exploration Bonus': mean_bonus\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comprehensive summary table (rounded to 3 decimal places for readability)\n",
    "    print(\"\\n===== AGENT COMPARISON SUMMARY =====\")\n",
    "    print(\"\\nPerformance Metrics by Agent:\")\n",
    "    display_df = comparison_df.round(3)\n",
    "    print(display_df)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Agent', y='Average Reward', data=comparison_df)\n",
    "    plt.title('Average Reward by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Agent', y='Optimal Actions (%)', data=comparison_df)\n",
    "    plt.title('Optimal Actions (%) by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Agent', y='Regret', data=comparison_df)\n",
    "    plt.title('Regret by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Exploration Bonus\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Agent', y='Exploration Bonus', data=comparison_df)\n",
    "    plt.title('Exploration Bonus by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Agent Performance Comparison\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate summary of best agent\n",
    "    print(\"\\nBest Agent (Based on Average Reward):\")\n",
    "    best_agent = comparison_df.loc[comparison_df['Average Reward'].idxmax()]\n",
    "    print(best_agent[['Agent', 'Average Reward']].to_string())\n",
    "\n",
    "\n",
    "# ================ AGENT FUNCTIONS ================\n",
    "\n",
    "\n",
    "def problem1_agent(env_info: Dict) -> int:\n",
    "    pass\n",
    "\n",
    "\n",
    "def problem2_agent(env_info: Dict) -> int:\n",
    "    pass\n",
    "\n",
    "def problem3_agent(env_info: Dict) -> int:\n",
    "    pass\n",
    "\n",
    "def problem4_agent(env_info: Dict) -> int:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_regularization_lambdas(n, low=1e-5, high=1e2, seed=None):\n",
    "    \"\"\"\n",
    "    Generate n lambda values from a log-uniform distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - n (int): Number of lambda values to generate.\n",
    "    - low (float): Lower bound of the range (must be > 0).\n",
    "    - high (float): Upper bound of the range.\n",
    "    - seed (int or None): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - lambdas (np.ndarray): Array of shape (n,) with sampled lambda values.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    log_low = np.log10(low)\n",
    "    log_high = np.log10(high)\n",
    "    lambdas = np.power(10, np.random.uniform(log_low, log_high, n))\n",
    "    return lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents & Core Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem1_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 1: Independent, Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent implements a simple belief state update using the known transition matrices\n",
    "    and combines exploitation with exploration via the exploration bonus.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - transition_matrices (List[np.ndarray]): Known transition matrices for both arms.\n",
    "                                                     Each is a 2D array of shape (state_count, state_count)\n",
    "                                                     where transition_matrices[i][s1, s2] is the probability\n",
    "                                                     of transitioning from state s1 to s2 for arm i.\n",
    "            - state_rewards (List[float]): Rewards associated with each state (e.g., [0.2, 0.8] for\n",
    "                                         \"bad\" and \"good\" states)\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = evaluate_problem1_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem1_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem1_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def problem2_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 2: Independent, Unknown Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent must learn the transition matrices through experience while also\n",
    "    balancing exploration and exploitation. Since the transition matrices are unknown,\n",
    "    the agent needs to maintain estimates of them based on observed rewards.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action (typically 0.0 or 1.0 \n",
    "                                         for Bernoulli rewards)\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Note: \n",
    "        Unlike Problem 1, no transition matrices are provided because they are unknown.\n",
    "        The agent must infer the hidden states and transition dynamics from the reward sequence.\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem2_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem2_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem2_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def problem3_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 3: Dependent (Joint), Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent must track the joint belief state across both arms using the known\n",
    "    joint transition matrix. The key difference from Problem 1 is that the states of\n",
    "    the two arms can be correlated, so pulling one arm may provide information about\n",
    "    the state of the other arm.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - joint_transition_matrix (np.ndarray): Known joint transition matrix of shape \n",
    "                                                 (joint_state_count, joint_state_count).\n",
    "                                                 For state_count=2, this will be a 4x4 matrix where\n",
    "                                                 the indices represent joint states: \n",
    "                                                 0=(0,0), 1=(0,1), 2=(1,0), 3=(1,1)\n",
    "            - state_count (int): Number of states per arm (e.g., 2 for \"good\"/\"bad\")\n",
    "            - state_rewards (List[float]): Rewards associated with each state (e.g., [0.2, 0.8])\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = evaluate_problem3_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem3_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem3_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem4_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 4: Partially Observable / Possibly Unknown / Possibly Random T\n",
    "    \n",
    "    This is the most general agent that must handle various scenarios:\n",
    "    - Transitions may be known or unknown\n",
    "    - Horizon may be fixed or random\n",
    "    - Arms may have independent or joint dynamics\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int, optional): Total number of turns. May be missing if random_horizon is True.\n",
    "            \n",
    "            # Transition information (may be present or absent):\n",
    "            - transition_matrices (List[np.ndarray], optional): If present, contains the known independent \n",
    "              transition matrices for both arms. Each matrix has shape (state_count, state_count).\n",
    "            - joint_transition_matrix (np.ndarray, optional): If present, contains the known joint \n",
    "              transition matrix of shape (joint_state_count, joint_state_count).\n",
    "            - joint_dynamics (bool, optional): If True, arms have joint dynamics; if False, they're independent.\n",
    "              Only present if transitions are known.\n",
    "            - state_rewards (List[float], optional): Rewards associated with each state. Only present \n",
    "              if transitions are known.\n",
    "            \n",
    "            # Horizon information:\n",
    "            - random_horizon (bool, optional): If True, the horizon is random and not fixed.\n",
    "            - discount_factor (float, optional): Discount factor for infinite/random horizon. Only \n",
    "              present if random_horizon is True.\n",
    "            \n",
    "            # History:\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \n",
    "    Note:\n",
    "        This is the most challenging scenario as it combines all variations:\n",
    "        - May need to learn transition dynamics if unknown\n",
    "        - May need to handle joint state tracking\n",
    "        - May need to use discounting for random/infinite horizon\n",
    "        - Always needs to balance exploration vs. exploitation\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem4_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem4_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem4_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_results = run_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_all_agents_with_varying_bonus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
